<!DOCTYPE html> <html lang=""> <head> <meta name="google-site-verification" content="5i6W-Df_KHh6DhAaV0PeyIlTpppRP8LwnrqK3LwrDnA"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Shota Horiguchi</title> <meta name="author" content="Shota Horiguchi"/> <meta name="description" content="Shota Horiguchi's personal webpage. "/> <meta name="keywords" content="Shota Horiguchi"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha256-1oz1+rx2BF/9OiBUPSh1NEUwyUECNU5O70RoKnClGNs=" crossorigin="anonymous"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@300;400;500;700&family=Roboto+Slab:wght@100;300;400;500;700&family=Roboto:wght@300;400;500;700&display=swap"> <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🐧</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://shota-horiguchi.github.io/ja/"> <title>概要 | Shota Horiguchi</title> <meta name="generator" content="Jekyll v4.2.2"/> <meta property="og:title" content="概要"/> <meta property="og:locale" content="ja"/> <meta name="description" content="Shota Horiguchi’s personal webpage."/> <meta property="og:description" content="Shota Horiguchi’s personal webpage."/> <link rel="canonical" href="https://shota-horiguchi.github.io/ja/"/> <meta property="og:url" content="https://shota-horiguchi.github.io/"/> <meta property="og:site_name" content="Shota Horiguchi"/> <meta property="og:type" content="website"/> <meta name="twitter:card" content="summary"/> <meta property="twitter:title" content="概要"/> <meta name="google-site-verification" content="5i6W-Df_KHh6DhAaV0PeyIlTpppRP8LwnrqK3LwrDnA"/> <script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"Shota Horiguchi’s personal webpage.","headline":"概要","name":"Shota Horiguchi","url":"https://shota-horiguchi.github.io/"}</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="https://orcid.org/0000-0002-3166-4956" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=9U5YK3wAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://www.webofscience.com/wos/author/record/36133050/" title="Web of Science" target="_blank" rel="noopener noreferrer"><i class="ai ai-clarivate"></i></a> <a href="https://www.researchgate.net/profile/Shota-Horiguchi/" title="ResearchGate" target="_blank" rel="noopener noreferrer"><i class="ai ai-researchgate"></i></a> <a href="https://www.linkedin.com/in/shota-horiguchi" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://dblp.org/pid/191/2558.html" title="DBLP" target="_blank" rel="noopener noreferrer"><i class="ai ai-dblp"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/ja/">概要<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/ja/publications/">発表文献</a> </li> <li class="nav-item "> <a class="nav-link" href="/ja/publications_ja/">発表文献（国内）</a> </li> <li class="nav-item "> <a class="nav-link" href="/ja/cv/">経歴</a> </li> <li class="nav-link nav-divider"> </li> <li class="nav-item dropdown"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"> <img class="emoji" title=":jp:" alt=":jp:" src="/assets/emoji/unicode/1f1ef-1f1f5.png" height="20" width="20"> 日本語</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/"><img class="emoji" title=":globe_with_meridians:" alt=":globe_with_meridians:" src="/assets/emoji/unicode/1f310.png" height="20" width="20"> English</a> <a class="dropdown-item" href="/ja/"><img class="emoji" title=":jp:" alt=":jp:" src="/assets/emoji/unicode/1f1ef-1f1f5.png" height="20" width="20"> 日本語</a> </div> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">堀口翔太</span> </h1></header> <p class="desc">horiguchi [at] ieee.org</p> <article> <div class="clearfix"> <p><a href="https://www.rd.ntt/hil/" target="_blank" rel="noopener noreferrer">NTT人間情報研究所</a>のリサーチスペシャリストで，音声関連技術の研究開発を行っています。</p> <p>2017年から2024年までは<a href="https://www.hitachi.co.jp/" target="_blank" rel="noopener noreferrer">株式会社日立製作所</a>の研究開発グループに所属していました。</p> <p>2023年に<a href="https://www.tsukuba.ac.jp/" target="_blank" rel="noopener noreferrer">筑波大学</a>から博士号を授与されました。在学中は<a href="https://www.mmlab.cs.tsukuba.ac.jp/" target="_blank" rel="noopener noreferrer">マルチメディア研究室</a>に所属し，<a href="https://www.mmlab.cs.tsukuba.ac.jp/~takeshi/" target="_blank" rel="noopener noreferrer">山田武志准教授</a>の指導を受けていました。</p> <p>2017年3月までは<a href="https://www.u-tokyo.ac.jp/ja/index.html" target="_blank" rel="noopener noreferrer">東京大学</a>の相澤・山﨑研究室（現・<a href="http://www.hal.t.u-tokyo.ac.jp/lab/" target="_blank" rel="noopener noreferrer">相澤・山肩・松井研究室</a>)にてコンピュータビジョンの研究を行っており，<a href="https://www.hal.t.u-tokyo.ac.jp/~aizawa/" target="_blank" rel="noopener noreferrer">相澤清晴教授</a>の指導の下，学士号と修士号を取得しました。</p> </div> <div class="news"> <h2>新着情報</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="white-space: nowrap">Dec 21, 2024</th> <td> <a href="https://arxiv.org/abs/2410.12182" target="_blank" rel="noopener noreferrer">主著1件</a>＆共著3件がICASSP 2025に採択されました。 </td> </tr> <tr> <th scope="row" style="white-space: nowrap">Dec 4, 2024</th> <td> <a href="https://arxiv.org/abs/2408.17142" target="_blank" rel="noopener noreferrer">主著論文</a>がIEEE SLT 2024にてHonorable Mention Awardを受賞しました。 </td> </tr> <tr> <th scope="row" style="white-space: nowrap">Aug 31, 2024</th> <td> <a href="https://arxiv.org/abs/2408.17142" target="_blank" rel="noopener noreferrer">主著1件</a>＆<a href="https://arxiv.org/abs/2410.11243" target="_blank" rel="noopener noreferrer">共著1件</a>がSLT 2024に採択されました。 </td> </tr> <tr> <th scope="row" style="white-space: nowrap">Jun 8, 2024</th> <td> 共著論文がINTERSPEECH 2024に2件採択されました。 </td> </tr> <tr> <th scope="row" style="white-space: nowrap">Feb 1, 2024</th> <td> 株式会社日立製作所を退職し，日本電信電話株式会社に入社しました。NTT人間情報研究所にてResearch Specialistとして音声関連技術の研究に従事します。 </td> </tr> </table> </div> </div> <div class="publications"> <h2>代表発表文献</h2> <div class="pubtype-1"> <ol class="bibliography"> <li> <div class="row first-author"> <div class="col-sm-2 abbr"><abbr class="badge">TASLP</abbr></div> <div id="horiguchi2023online" class="col-sm-8"> <div class="title">Online Neural Diarization of Unlimited Numbers of Speakers Using Global and Local Attractors</div> <div class="author"> <em>Shota Horiguchi</em>, Shinji Watanabe, Paola Garcia, Yuki Takashima, and Yohei Kawaguchi</div> <div class="periodical"> <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, Jan 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2206.02432" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10003998" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>A method to perform offline and online speaker diarization for an unlimited number of speakers is described in this paper. End-to-end neural diarization (EEND) has achieved overlap-aware speaker diarization by formulating it as a multi-label classification problem. It has also been extended for a flexible number of speakers by introducing speaker-wise attractors. However, the output number of speakers of attractor-based EEND is empirically capped; it cannot deal with cases where the number of speakers appearing during inference is higher than that during training because its speaker counting is trained in a fully supervised manner. Our method, EEND-GLA, solves this problem by introducing unsupervised clustering into attractor-based EEND. In the method, the input audio is first divided into short blocks, then attractor-based diarization is performed for each block, and finally the results of each blocks are clustered on the basis of the similarity between locally-calculated attractors. While the number of output speakers is limited within each block, the total number of speakers estimated for the entire input can be higher than the limitation. To use EEND-GLA in an online manner, our method also extends the speaker-tracing buffer, which was originally proposed to enable online inference of conventional EEND. We introduce a block-wise buffer update to make the speaker-tracing buffer compatible with EEND-GLA. Finally, to improve online diarization, our method improves the buffer update method and revisits the variable chunk-size training of EEND. The experimental results demonstrate that EEND-GLA can perform speaker diarization of an unseen number of speakers in both offline and online inferences.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">horiguchi2023online</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Online Neural Diarization of Unlimited Numbers of Speakers Using Global and Local Attractors}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE/ACM Transactions on Audio, Speech, and Language Processing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Horiguchi, Shota and Watanabe, Shinji and Garcia, Paola and Takashima, Yuki and Kawaguchi, Yohei}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{31}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{706-720}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row first-author"> <div class="col-sm-2 abbr"><abbr class="badge">TASLP</abbr></div> <div id="horiguchi2022encoderdecoder" class="col-sm-8"> <div class="title">Encoder-Decoder Based Attractors for End-to-End Neural Diarization</div> <div class="author"> <em>Shota Horiguchi</em>, Yusuke Fujita, Shinji Watanabe, Yawen Xue, and Paola García</div> <div class="periodical"> <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, Mar 2022 </div> <div class="comment"> 🏆 Itakura Prize Innovative Young Researcher Award </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2106.10654" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9741374" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>This paper investigates an end-to-end neural diarization (EEND) method for an unknown number of speakers. In contrast to the conventional cascaded approach to speaker diarization, EEND methods are better in terms of speaker overlap handling. However, EEND still has a disadvantage in that it cannot deal with a flexible number of speakers. To remedy this problem, we introduce encoder-decoder-based attractor calculation module (EDA) to EEND. Once frame-wise embeddings are obtained, EDA sequentially generates speaker-wise attractors on the basis of a sequence-to-sequence method using an LSTM encoder-decoder. The attractor generation continues until a stopping condition is satisfied; thus, the number of attractors can be flexible. Diarization results are then estimated as dot products of the attractors and embeddings. The embeddings from speaker overlaps result in larger dot product values with multiple attractors; thus, this method can deal with speaker overlaps. Because the maximum number of output speakers is still limited by the training set, we also propose an iterative inference method to remove this restriction. Further, we propose a method that aligns the estimated diarization results with the results of an external speech activity detector, which enables fair comparison against cascaded approaches. Extensive evaluations on simulated and real datasets show that EEND-EDA outperforms the conventional cascaded approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">horiguchi2022encoderdecoder</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Encoder-Decoder Based Attractors for End-to-End Neural Diarization}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE/ACM Transactions on Audio, Speech, and Language Processing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Horiguchi, Shota and Fujita, Yusuke and Watanabe, Shinji and Xue, Yawen and Garc{\'i}a, Paola}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1493--1507}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row first-author"> <div class="col-sm-2 abbr"><abbr class="badge">TPAMI</abbr></div> <div id="horiguchi2020significance" class="col-sm-8"> <div class="title">Significance of Softmax-based Features in Comparison to Distance Metric Learning-based Features</div> <div class="author"> <em>Shota Horiguchi</em>, Daiki Ikami, and Kiyoharu Aizawa</div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, May 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1712.10151" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/8691614" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>End-to-end distance metric learning (DML) has been applied to obtain features useful in many computer vision tasks. However, these DML studies have not provided equitable comparisons between features extracted from DML-based networks and softmax-based networks. In this paper, we present objective comparisons between these two approaches under the same network architecture.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">horiguchi2020significance</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Significance of Softmax-based Features in Comparison to Distance Metric Learning-based Features}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Pattern Analysis and Machine Intelligence}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Horiguchi, Shota and Ikami, Daiki and Aizawa, Kiyoharu}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{42}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1279--1285}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row first-author"> <div class="col-sm-2 abbr"><abbr class="badge">TMM</abbr></div> <div id="horiguchi2018personalized" class="col-sm-8"> <div class="title">Personalized Classifier for Food Image Recognition</div> <div class="author"> <em>Shota Horiguchi</em>, Sosuke Amano, Makoto Ogawa, and Kiyoharu Aizawa</div> <div class="periodical"> <em>IEEE Transactions on Multimedia</em>, Oct 2018 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1804.04600" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/8316919" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Currently, food image recognition tasks are evaluated against fixed datasets. However, in real-world conditions, there are cases in which the number of samples in each class continues to increase and samples from novel classes appear. In particular, dynamic datasets in which each individual user creates samples and continues the updating process often has content that varies considerably between different users, and the number of samples per person is very limited. A single classifier common to all users cannot handle such dynamic data. Bridging the gap between the laboratory environment and the real world has not yet been accomplished on a large scale. Personalizing a classifier incrementally for each user is a promising way to do this. In this paper, we address the personalization problem, which involves adapting to the user’s domain incrementally using a very limited number of samples. We propose a simple yet effective personalization framework, which is a combination of the nearest class mean classifier and the 1-nearest neighbor classifier based on deep features. To conduct realistic experiments, we made use of a new dataset of daily food images collected by a food-logging application. Experimental results show that our proposed method significantly outperforms existing methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">horiguchi2018personalized</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Personalized Classifier for Food Image Recognition}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Multimedia}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Horiguchi, Shota and Amano, Sosuke and Ogawa, Makoto and Aizawa, Kiyoharu}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{20}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2836--2848}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </div> <div class="social"> <div class="contact-icons"> <a href="https://orcid.org/0000-0002-3166-4956" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=9U5YK3wAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://www.webofscience.com/wos/author/record/36133050/" title="Web of Science" target="_blank" rel="noopener noreferrer"><i class="ai ai-clarivate"></i></a> <a href="https://www.researchgate.net/profile/Shota-Horiguchi/" title="ResearchGate" target="_blank" rel="noopener noreferrer"><i class="ai ai-researchgate"></i></a> <a href="https://www.linkedin.com/in/shota-horiguchi" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://dblp.org/pid/191/2558.html" title="DBLP" target="_blank" rel="noopener noreferrer"><i class="ai ai-dblp"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2025 Shota Horiguchi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Last updated: February 17, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-throttle-debounce/1.1/jquery.ba-throttle-debounce.js"></script> <script async src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>