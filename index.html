<!DOCTYPE html> <html lang=""> <head> <meta name="google-site-verification" content="5i6W-Df_KHh6DhAaV0PeyIlTpppRP8LwnrqK3LwrDnA"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Shota Horiguchi</title> <meta name="author" content="Shota Horiguchi"/> <meta name="description" content="Shota Horiguchi's personal webpage. "/> <meta name="keywords" content="Shota Horiguchi"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha256-1oz1+rx2BF/9OiBUPSh1NEUwyUECNU5O70RoKnClGNs=" crossorigin="anonymous"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@300;400;500;700&family=Roboto+Slab:wght@100;300;400;500;700&family=Roboto:wght@300;400;500;700&display=swap"> <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üêß</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://shota-horiguchi.github.io/"> <title>about | Shota Horiguchi</title> <meta name="generator" content="Jekyll v4.2.2"/> <meta property="og:title" content="about"/> <meta property="og:locale" content="en"/> <meta name="description" content="Shota Horiguchi‚Äôs personal webpage."/> <meta property="og:description" content="Shota Horiguchi‚Äôs personal webpage."/> <link rel="canonical" href="https://shota-horiguchi.github.io/"/> <meta property="og:url" content="https://shota-horiguchi.github.io/"/> <meta property="og:site_name" content="Shota Horiguchi"/> <meta property="og:type" content="website"/> <meta name="twitter:card" content="summary"/> <meta property="twitter:title" content="about"/> <meta name="google-site-verification" content="5i6W-Df_KHh6DhAaV0PeyIlTpppRP8LwnrqK3LwrDnA"/> <script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"Shota Horiguchi‚Äôs personal webpage.","headline":"about","name":"Shota Horiguchi","url":"https://shota-horiguchi.github.io/"}</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="https://orcid.org/0000-0002-3166-4956" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=9U5YK3wAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://www.webofscience.com/wos/author/record/36133050/" title="Web of Science" target="_blank" rel="noopener noreferrer"><i class="ai ai-clarivate"></i></a> <a href="https://www.researchgate.net/profile/Shota-Horiguchi/" title="ResearchGate" target="_blank" rel="noopener noreferrer"><i class="ai ai-researchgate"></i></a> <a href="https://www.linkedin.com/in/shota-horiguchi" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://dblp.org/pid/191/2558.html" title="DBLP" target="_blank" rel="noopener noreferrer"><i class="ai ai-dblp"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications_ja/">publications (ja)</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-link nav-divider"> </li> <li class="nav-item dropdown"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><img class="emoji" title=":globe_with_meridians:" alt=":globe_with_meridians:" src="/assets/emoji/unicode/1f310.png" height="20" width="20"> English</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/"><img class="emoji" title=":globe_with_meridians:" alt=":globe_with_meridians:" src="/assets/emoji/unicode/1f310.png" height="20" width="20"> English</a> <a class="dropdown-item" href="/ja/"><img class="emoji" title=":jp:" alt=":jp:" src="/assets/emoji/unicode/1f1ef-1f1f5.png" height="20" width="20"> Êó•Êú¨Ë™û</a> </div> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Shota</span> Horiguchi</h1> <p class="desc">horiguchi [at] ieee.org</p> </header> <article> <div class="clearfix"> <p>I am a Research Specialist at <a href="https://www.rd.ntt/e/hil/" target="_blank" rel="noopener noreferrer">NTT Human Informatics Laboratories</a>. My research interests are not limited to speech-related technology such as speech recognition, speech separation, and speaker diarization, but also include image processing and multimodal processing.</p> <p>Prior to joining NTT, I was a Senior Researcher at <a href="https://www.hitachi.com/" target="_blank" rel="noopener noreferrer">Hitachi, Ltd.</a> Research and Development Group from 2017 to 2024.</p> <p>I received my Ph.D. degree in 2023 from <a href="https://www.tsukuba.ac.jp/en/" target="_blank" rel="noopener noreferrer">University of Tsukuba</a> under the supervision of <a href="https://www.mmlab.cs.tsukuba.ac.jp/~takeshi/english/" target="_blank" rel="noopener noreferrer">Prof. Takeshi Yamada</a>. At University of Tsukuba, I was a member of the <a href="https://www.mmlab.cs.tsukuba.ac.jp/english/" target="_blank" rel="noopener noreferrer">Multimedia Laboratory</a>.</p> <p>Before that, I studied computer vision at the Aizawa-Yamasaki Laboratory (now <a href="http://www.hal.t.u-tokyo.ac.jp/lab/en/" target="_blank" rel="noopener noreferrer">Aizawa-Yamakata-Matsui Laboratory</a>) of <a href="https://www.u-tokyo.ac.jp/en/index.html" target="_blank" rel="noopener noreferrer">the University of Tokyo</a>, where I received B.E. and M.E. degrees under the supervision of <a href="https://www.hal.t.u-tokyo.ac.jp/~aizawa/" target="_blank" rel="noopener noreferrer">Prof. Kiyoharu Aizawa</a>.</p> </div> <div class="news"> <h2>news</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="white-space: nowrap">Dec 21, 2024</th> <td> <a href="https://arxiv.org/abs/2410.12182" target="_blank" rel="noopener noreferrer">One first-authored paper</a> and three co-authored papers have been accepted to ICASSP 2025. </td> </tr> <tr> <th scope="row" style="white-space: nowrap">Dec 4, 2024</th> <td> <a href="https://arxiv.org/abs/2408.17142" target="_blank" rel="noopener noreferrer">Our paper</a> won the Honorable Mention Award at IEEE SLT 2024. </td> </tr> <tr> <th scope="row" style="white-space: nowrap">Aug 31, 2024</th> <td> <a href="https://arxiv.org/abs/2408.17142" target="_blank" rel="noopener noreferrer">One first-authored paper</a> and <a href="https://arxiv.org/abs/2410.11243" target="_blank" rel="noopener noreferrer">one co-authored paper</a> have been accepted to SLT 2024. </td> </tr> <tr> <th scope="row" style="white-space: nowrap">Jun 8, 2024</th> <td> Two co-authored papers have been accepted to INTERSPEECH 2024. </td> </tr> <tr> <th scope="row" style="white-space: nowrap">Feb 1, 2024</th> <td> I‚Äôm starting a new position as Research Specialist at NTT Human Informatics Laboratories. </td> </tr> </table> </div> </div> <div class="publications"> <h2>selected publications</h2> <div class="pubtype-1"> <ol class="bibliography"> <li> <div class="row first-author"> <div class="col-sm-2 abbr"><abbr class="badge">TASLP</abbr></div> <div id="horiguchi2023online" class="col-sm-8"> <div class="title">Online Neural Diarization of Unlimited Numbers of Speakers Using Global and Local Attractors</div> <div class="author"> <em>Shota Horiguchi</em>,¬†Shinji Watanabe,¬†Paola Garcia,¬†Yuki Takashima,¬†and Yohei Kawaguchi</div> <div class="periodical"> <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, Jan 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2206.02432" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10003998" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>A method to perform offline and online speaker diarization for an unlimited number of speakers is described in this paper. End-to-end neural diarization (EEND) has achieved overlap-aware speaker diarization by formulating it as a multi-label classification problem. It has also been extended for a flexible number of speakers by introducing speaker-wise attractors. However, the output number of speakers of attractor-based EEND is empirically capped; it cannot deal with cases where the number of speakers appearing during inference is higher than that during training because its speaker counting is trained in a fully supervised manner. Our method, EEND-GLA, solves this problem by introducing unsupervised clustering into attractor-based EEND. In the method, the input audio is first divided into short blocks, then attractor-based diarization is performed for each block, and finally the results of each blocks are clustered on the basis of the similarity between locally-calculated attractors. While the number of output speakers is limited within each block, the total number of speakers estimated for the entire input can be higher than the limitation. To use EEND-GLA in an online manner, our method also extends the speaker-tracing buffer, which was originally proposed to enable online inference of conventional EEND. We introduce a block-wise buffer update to make the speaker-tracing buffer compatible with EEND-GLA. Finally, to improve online diarization, our method improves the buffer update method and revisits the variable chunk-size training of EEND. The experimental results demonstrate that EEND-GLA can perform speaker diarization of an unseen number of speakers in both offline and online inferences.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">horiguchi2023online</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Online Neural Diarization of Unlimited Numbers of Speakers Using Global and Local Attractors}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE/ACM Transactions on Audio, Speech, and Language Processing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Horiguchi, Shota and Watanabe, Shinji and Garcia, Paola and Takashima, Yuki and Kawaguchi, Yohei}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{31}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{706-720}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row first-author"> <div class="col-sm-2 abbr"><abbr class="badge">TASLP</abbr></div> <div id="horiguchi2022encoderdecoder" class="col-sm-8"> <div class="title">Encoder-Decoder Based Attractors for End-to-End Neural Diarization</div> <div class="author"> <em>Shota Horiguchi</em>,¬†Yusuke Fujita,¬†Shinji Watanabe,¬†Yawen Xue,¬†and Paola Garc√≠a</div> <div class="periodical"> <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, Mar 2022 </div> <div class="comment"> üèÜ Itakura Prize Innovative Young Researcher Award </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2106.10654" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9741374" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>This paper investigates an end-to-end neural diarization (EEND) method for an unknown number of speakers. In contrast to the conventional cascaded approach to speaker diarization, EEND methods are better in terms of speaker overlap handling. However, EEND still has a disadvantage in that it cannot deal with a flexible number of speakers. To remedy this problem, we introduce encoder-decoder-based attractor calculation module (EDA) to EEND. Once frame-wise embeddings are obtained, EDA sequentially generates speaker-wise attractors on the basis of a sequence-to-sequence method using an LSTM encoder-decoder. The attractor generation continues until a stopping condition is satisfied; thus, the number of attractors can be flexible. Diarization results are then estimated as dot products of the attractors and embeddings. The embeddings from speaker overlaps result in larger dot product values with multiple attractors; thus, this method can deal with speaker overlaps. Because the maximum number of output speakers is still limited by the training set, we also propose an iterative inference method to remove this restriction. Further, we propose a method that aligns the estimated diarization results with the results of an external speech activity detector, which enables fair comparison against cascaded approaches. Extensive evaluations on simulated and real datasets show that EEND-EDA outperforms the conventional cascaded approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">horiguchi2022encoderdecoder</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Encoder-Decoder Based Attractors for End-to-End Neural Diarization}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE/ACM Transactions on Audio, Speech, and Language Processing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Horiguchi, Shota and Fujita, Yusuke and Watanabe, Shinji and Xue, Yawen and Garc{\'i}a, Paola}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1493--1507}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row first-author"> <div class="col-sm-2 abbr"><abbr class="badge">TPAMI</abbr></div> <div id="horiguchi2020significance" class="col-sm-8"> <div class="title">Significance of Softmax-based Features in Comparison to Distance Metric Learning-based Features</div> <div class="author"> <em>Shota Horiguchi</em>,¬†Daiki Ikami,¬†and Kiyoharu Aizawa</div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, May 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1712.10151" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/8691614" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>End-to-end distance metric learning (DML) has been applied to obtain features useful in many computer vision tasks. However, these DML studies have not provided equitable comparisons between features extracted from DML-based networks and softmax-based networks. In this paper, we present objective comparisons between these two approaches under the same network architecture.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">horiguchi2020significance</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Significance of Softmax-based Features in Comparison to Distance Metric Learning-based Features}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Pattern Analysis and Machine Intelligence}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Horiguchi, Shota and Ikami, Daiki and Aizawa, Kiyoharu}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{42}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1279--1285}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row first-author"> <div class="col-sm-2 abbr"><abbr class="badge">TMM</abbr></div> <div id="horiguchi2018personalized" class="col-sm-8"> <div class="title">Personalized Classifier for Food Image Recognition</div> <div class="author"> <em>Shota Horiguchi</em>,¬†Sosuke Amano,¬†Makoto Ogawa,¬†and Kiyoharu Aizawa</div> <div class="periodical"> <em>IEEE Transactions on Multimedia</em>, Oct 2018 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1804.04600" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/8316919" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Currently, food image recognition tasks are evaluated against fixed datasets. However, in real-world conditions, there are cases in which the number of samples in each class continues to increase and samples from novel classes appear. In particular, dynamic datasets in which each individual user creates samples and continues the updating process often has content that varies considerably between different users, and the number of samples per person is very limited. A single classifier common to all users cannot handle such dynamic data. Bridging the gap between the laboratory environment and the real world has not yet been accomplished on a large scale. Personalizing a classifier incrementally for each user is a promising way to do this. In this paper, we address the personalization problem, which involves adapting to the user‚Äôs domain incrementally using a very limited number of samples. We propose a simple yet effective personalization framework, which is a combination of the nearest class mean classifier and the 1-nearest neighbor classifier based on deep features. To conduct realistic experiments, we made use of a new dataset of daily food images collected by a food-logging application. Experimental results show that our proposed method significantly outperforms existing methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">horiguchi2018personalized</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Personalized Classifier for Food Image Recognition}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Multimedia}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Horiguchi, Shota and Amano, Sosuke and Ogawa, Makoto and Aizawa, Kiyoharu}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{20}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2836--2848}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </div> <div class="social"> <div class="contact-icons"> <a href="https://orcid.org/0000-0002-3166-4956" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=9U5YK3wAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://www.webofscience.com/wos/author/record/36133050/" title="Web of Science" target="_blank" rel="noopener noreferrer"><i class="ai ai-clarivate"></i></a> <a href="https://www.researchgate.net/profile/Shota-Horiguchi/" title="ResearchGate" target="_blank" rel="noopener noreferrer"><i class="ai ai-researchgate"></i></a> <a href="https://www.linkedin.com/in/shota-horiguchi" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://dblp.org/pid/191/2558.html" title="DBLP" target="_blank" rel="noopener noreferrer"><i class="ai ai-dblp"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> ¬© Copyright 2025 Shota Horiguchi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Last updated: February 17, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-throttle-debounce/1.1/jquery.ba-throttle-debounce.js"></script> <script async src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>